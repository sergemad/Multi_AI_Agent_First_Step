{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from sentence-transformers) (4.66.4)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.2.2-cp39-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from sentence-transformers) (1.26.4)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.5.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.15.1 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from sentence-transformers) (0.23.2)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-10.3.0-cp39-cp39-macosx_10_10_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: filelock in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Using cached safetensors-0.4.3-cp39-cp39-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.0.0-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.7/224.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp39-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "Using cached pillow-10.3.0-cp39-cp39-macosx_10_10_x86_64.whl (3.5 MB)\n",
      "Downloading scikit_learn-1.5.0-cp39-cp39-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp39-cp39-macosx_10_9_x86_64.whl (39.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.4/39.4 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached safetensors-0.4.3-cp39-cp39-macosx_10_12_x86_64.whl (416 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, safetensors, Pillow, networkx, joblib, torch, scikit-learn, transformers, sentence-transformers\n",
      "Successfully installed Pillow-10.3.0 joblib-1.4.2 networkx-3.2.1 safetensors-0.4.3 scikit-learn-1.5.0 scipy-1.13.1 sentence-transformers-3.0.0 threadpoolctl-3.5.0 torch-2.2.2 transformers-4.41.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install pypdf\n",
    "#!pip install chromadb\n",
    "#!pip install gpt4all\n",
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from gpt4all import Embed4All\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "import getpass\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass(f\"Please provide your {var}\")\n",
    "\n",
    "_set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"AI Assistant\"\n",
    "\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    #embeddings = BedrockEmbeddings(credentials_profile_name=\"default\", region_name=\"us-east-1\")\n",
    "    #embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "\n",
    "    # This will create IDs like \"data/monopoly.pdf:6:2\"\n",
    "    # Page Source : Page Number : Chunk Index\n",
    "\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # If the page ID is the same as the last one, increment the index.\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Calculate the chunk ID.\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        # Add it to the page meta-data.\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH, embedding_function=get_embedding_function()\n",
    "    )\n",
    "\n",
    "    # Calculate Page IDs.\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # Add or Update the documents.\n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # Only add documents that don't exist in the DB.\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"👉 Adding new documents: {len(new_chunks)}\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        db.persist()\n",
    "    else:\n",
    "        print(\"✅ No new documents to add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_database():\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing documents in DB: 0\n",
      "👉 Adding new documents: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "add_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text: str):\n",
    "    # Prepare the DB.\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    # print(prompt)\n",
    "\n",
    "    model = Ollama(model=\"openhermes\")\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Serge à t il des compétence en python et AWS ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Number of requested results 5 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Oui, Serge a des compétences en Python et AWS. Il est mentionné qu'il a travaillé comme Développeur Python & Rust à LEDR Technologies et est certifié AWS Solutions Architect Associate. Il possède également des expériences professionnelles impliquant le déploiement de solutions informatiques sur AWS et l'utilisation d'outils tels que Terraform pour la conception d'architectures AWS.\n",
      "Sources: ['data/CV Serge Keita.pdf:0:0', 'data/CV Serge Keita.pdf:0:1', 'data/CV Serge Keita.pdf:0:2', 'data/CV Serge Keita.pdf:0:3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Oui, Serge a des compétences en Python et AWS. Il est mentionné qu'il a travaillé comme Développeur Python & Rust à LEDR Technologies et est certifié AWS Solutions Architect Associate. Il possède également des expériences professionnelles impliquant le déploiement de solutions informatiques sur AWS et l'utilisation d'outils tels que Terraform pour la conception d'architectures AWS.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag2(query_text: str):\n",
    "    # Prepare the DB.\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    # print(prompt)\n",
    "\n",
    "    #pipeline(\"text-generation\", model=\"tiiuae/falcon-7b-instruct\", trust_remote_code=True)\n",
    "    llm = pipeline(\"text-generation\", model=\"gpt2\", framework=\"pt\")  # force PyTorch framework\n",
    "\n",
    "    #model = Ollama(model=\"openhermes\")\n",
    "    #response_text = model.invoke(prompt)\n",
    "    response = llm(prompt, max_length=1000)\n",
    "    response_text = response[0][\"generated_text\"]\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 4, updating n_results = 4\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "COMPÉTENCESS E R G E\n",
      "KEITAData Enthousiaste | Machine Learning | Devops\n",
      "Cloud | Python Développeur\n",
      "POINTS FORTS+33698480507 @  sergekeita01@gmail.com \n",
      "CERTIFICATIONS\n",
      "AWS Cloud Quest : Machine\n",
      "Learning\n",
      "AWS Solutions Architect Associate \n",
      "HarshiCorp Terraform Associate 003\n",
      "Certification CISCO CCNA1 \n",
      "Certification AZ 900 (2022)\n",
      "TOEICData Scientest\n",
      "English (Fluent)\n",
      "French (Fluent)LANGUAGESEXPERIENCES\n",
      "FORMATIONSPROFILE\n",
      "Fort d'une expérience internationale et d’un master en Big Data & Artificial\n",
      "Intelligence, j'ai acquis de solides connaissances en programmation, machine\n",
      "learning et le déploiement de solutions informatiques sur AWS. Passionné par\n",
      "la data, j'aime explorer de nouvelles idées et proposer des solutions créatives\n",
      "pour relever les défis du domaine.\n",
      "\n",
      "---\n",
      "\n",
      "pour relever les défis du domaine.\n",
      "12/2023 - 04/2024 Consultant Devops Cloud AWS\n",
      "Data Scientest X Storm Reply - POEI\n",
      "02/2023 - 08/2023 Consultant Data & Cyber\n",
      "HEADMIND PARTNERS\n",
      "08/2022 - 01/2023 Data Scientist\n",
      "DATAXLABConception d'architectures AWS pour le déploiement de services web\n",
      "Terraform & services AWS : ECS, RDS, CLOUDFRONT, ALB, EFS, S3 ...\n",
      "GitHub Action & Pytest\n",
      "Kubernetes / Docker / FastAPI & Linux\n",
      "Programmation d’un outil d’analyse de risque sécurité/d’audit\n",
      "Implémentation et entrainement d'un model de classification de\n",
      "fichier confidentiel DLP \n",
      "Création d’un model d'analyse de risque sécurité basé sur du Natural\n",
      "Language Processing (NLP) \n",
      "Développement d’un model pour de la détection de bactéries.\n",
      "Collecte, nettoyage et préparation des données\n",
      "\n",
      "---\n",
      "\n",
      "Collecte, nettoyage et préparation des données\n",
      "Création du model de segmentation d'image non supervisé (WNET)\n",
      "Analyse et présentation des résultats du model - Anglais\n",
      "06/2022  - 08/2022 Développeur Python & Rust\n",
      "LEDR TECHNOLOGIES\n",
      "Programmation de fonctions en Python\n",
      "Migration d’une grande partie des fonctions en RustPARIS\n",
      "PARIS\n",
      "LAS VEGAS\n",
      "LOS ANGELES\n",
      "EFREI Paris\n",
      "Université Claude Bernard Lyon 1Devops Cloud Engineer\n",
      "Big Data & AI Engineer\n",
      "Lincence Informatique2023 - 2024\n",
      "PARIS\n",
      "2020 - 2023\n",
      "PARIS\n",
      "2017 - 2020\n",
      "LYONProgrammations : Python, Scala,\n",
      "Java, Rust\n",
      "Machine/Deep learning  : NLP,\n",
      "KNN, K-means, SVM, Linear &\n",
      "logistic regression ...\n",
      "SQL & NoSQL : MySQL,\n",
      "MariaDB, MongoDB \n",
      "FastAPI, Administration linux\n",
      "GitHub & Github Action / Gitlab\n",
      "Docker, Kubernetes, Ansible,\n",
      "Terraform, AWS\n",
      "\n",
      "---\n",
      "\n",
      "GitHub & Github Action / Gitlab\n",
      "Docker, Kubernetes, Ansible,\n",
      "Terraform, AWS\n",
      "MLflow, Streamlit, Gen AITravail d’équipeRésolution de problème\n",
      "Collaborer en équipe pour\n",
      "mettre en œuvre des solutions\n",
      "datacomprendre les besoins métier\n",
      "et proposer des solutions\n",
      "techniques appropriéesCapacité d’analyse\n",
      "capacité à analyser des données\n",
      "complexes, à identifier des\n",
      "tendances significativesp o r t f o l i o\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: Serge à t il des compétence en python et AWS ?\n",
      "\n",
      "FORTRANO NATIONAL ASSOCIATION\n",
      "\n",
      "AWS, AWS - Automata, Big Data Dev\n",
      "Sources: ['data/CV Serge Keita.pdf:0:0', 'data/CV Serge Keita.pdf:0:1', 'data/CV Serge Keita.pdf:0:2', 'data/CV Serge Keita.pdf:0:3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Human: \\nAnswer the question based only on the following context:\\n\\nCOMPÉTENCESS E R G E\\nKEITAData Enthousiaste | Machine Learning | Devops\\nCloud | Python Développeur\\nPOINTS FORTS+33698480507 @  sergekeita01@gmail.com \\nCERTIFICATIONS\\nAWS Cloud Quest : Machine\\nLearning\\nAWS Solutions Architect Associate \\nHarshiCorp Terraform Associate 003\\nCertification CISCO CCNA1 \\nCertification AZ 900 (2022)\\nTOEICData Scientest\\nEnglish (Fluent)\\nFrench (Fluent)LANGUAGESEXPERIENCES\\nFORMATIONSPROFILE\\nFort d'une expérience internationale et d’un master en Big Data & Artificial\\nIntelligence, j'ai acquis de solides connaissances en programmation, machine\\nlearning et le déploiement de solutions informatiques sur AWS. Passionné par\\nla data, j'aime explorer de nouvelles idées et proposer des solutions créatives\\npour relever les défis du domaine.\\n\\n---\\n\\npour relever les défis du domaine.\\n12/2023 - 04/2024 Consultant Devops Cloud AWS\\nData Scientest X Storm Reply - POEI\\n02/2023 - 08/2023 Consultant Data & Cyber\\nHEADMIND PARTNERS\\n08/2022 - 01/2023 Data Scientist\\nDATAXLABConception d'architectures AWS pour le déploiement de services web\\nTerraform & services AWS : ECS, RDS, CLOUDFRONT, ALB, EFS, S3 ...\\nGitHub Action & Pytest\\nKubernetes / Docker / FastAPI & Linux\\nProgrammation d’un outil d’analyse de risque sécurité/d’audit\\nImplémentation et entrainement d'un model de classification de\\nfichier confidentiel DLP \\nCréation d’un model d'analyse de risque sécurité basé sur du Natural\\nLanguage Processing (NLP) \\nDéveloppement d’un model pour de la détection de bactéries.\\nCollecte, nettoyage et préparation des données\\n\\n---\\n\\nCollecte, nettoyage et préparation des données\\nCréation du model de segmentation d'image non supervisé (WNET)\\nAnalyse et présentation des résultats du model - Anglais\\n06/2022  - 08/2022 Développeur Python & Rust\\nLEDR TECHNOLOGIES\\nProgrammation de fonctions en Python\\nMigration d’une grande partie des fonctions en RustPARIS\\nPARIS\\nLAS VEGAS\\nLOS ANGELES\\nEFREI Paris\\nUniversité Claude Bernard Lyon 1Devops Cloud Engineer\\nBig Data & AI Engineer\\nLincence Informatique2023 - 2024\\nPARIS\\n2020 - 2023\\nPARIS\\n2017 - 2020\\nLYONProgrammations : Python, Scala,\\nJava, Rust\\nMachine/Deep learning  : NLP,\\nKNN, K-means, SVM, Linear &\\nlogistic regression ...\\nSQL & NoSQL : MySQL,\\nMariaDB, MongoDB \\nFastAPI, Administration linux\\nGitHub & Github Action / Gitlab\\nDocker, Kubernetes, Ansible,\\nTerraform, AWS\\n\\n---\\n\\nGitHub & Github Action / Gitlab\\nDocker, Kubernetes, Ansible,\\nTerraform, AWS\\nMLflow, Streamlit, Gen AITravail d’équipeRésolution de problème\\nCollaborer en équipe pour\\nmettre en œuvre des solutions\\ndatacomprendre les besoins métier\\net proposer des solutions\\ntechniques appropriéesCapacité d’analyse\\ncapacité à analyser des données\\ncomplexes, à identifier des\\ntendances significativesp o r t f o l i o\\n\\n---\\n\\nAnswer the question based on the above context: Serge à t il des compétence en python et AWS ?\\n\\nFORTRANO NATIONAL ASSOCIATION\\n\\nAWS, AWS - Automata, Big Data Dev\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag2(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
