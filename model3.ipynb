{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from sentence-transformers) (4.66.4)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.2.2-cp39-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from sentence-transformers) (1.26.4)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.5.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.15.1 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from sentence-transformers) (0.23.2)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-10.3.0-cp39-cp39-macosx_10_10_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: filelock in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Using cached safetensors-0.4.3-cp39-cp39-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.0.0-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m224.7/224.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp39-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "Using cached pillow-10.3.0-cp39-cp39-macosx_10_10_x86_64.whl (3.5 MB)\n",
      "Downloading scikit_learn-1.5.0-cp39-cp39-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp39-cp39-macosx_10_9_x86_64.whl (39.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.4/39.4 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached safetensors-0.4.3-cp39-cp39-macosx_10_12_x86_64.whl (416 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, safetensors, Pillow, networkx, joblib, torch, scikit-learn, transformers, sentence-transformers\n",
      "Successfully installed Pillow-10.3.0 joblib-1.4.2 networkx-3.2.1 safetensors-0.4.3 scikit-learn-1.5.0 scipy-1.13.1 sentence-transformers-3.0.0 threadpoolctl-3.5.0 torch-2.2.2 transformers-4.41.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install pypdf\n",
    "#!pip install chromadb\n",
    "#!pip install gpt4all\n",
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from gpt4all import Embed4All\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "import getpass\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass(f\"Please provide your {var}\")\n",
    "\n",
    "_set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"AI Assistant\"\n",
    "\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    #embeddings = BedrockEmbeddings(credentials_profile_name=\"default\", region_name=\"us-east-1\")\n",
    "    #embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "\n",
    "    # This will create IDs like \"data/monopoly.pdf:6:2\"\n",
    "    # Page Source : Page Number : Chunk Index\n",
    "\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # If the page ID is the same as the last one, increment the index.\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Calculate the chunk ID.\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        # Add it to the page meta-data.\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH, embedding_function=get_embedding_function()\n",
    "    )\n",
    "\n",
    "    # Calculate Page IDs.\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # Add or Update the documents.\n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # Only add documents that don't exist in the DB.\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"üëâ Adding new documents: {len(new_chunks)}\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        db.persist()\n",
    "    else:\n",
    "        print(\"‚úÖ No new documents to add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_database():\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing documents in DB: 0\n",
      "üëâ Adding new documents: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "add_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text: str):\n",
    "    # Prepare the DB.\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    # print(prompt)\n",
    "\n",
    "    model = Ollama(model=\"openhermes\")\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Serge √† t il des comp√©tence en python et AWS ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergekeita/opt/anaconda3/envs/llm_langgraph/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Number of requested results 5 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Oui, Serge a des comp√©tences en Python et AWS. Il est mentionn√© qu'il a travaill√© comme D√©veloppeur Python & Rust √† LEDR Technologies et est certifi√© AWS Solutions Architect Associate. Il poss√®de √©galement des exp√©riences professionnelles impliquant le d√©ploiement de solutions informatiques sur AWS et l'utilisation d'outils tels que Terraform pour la conception d'architectures AWS.\n",
      "Sources: ['data/CV Serge Keita.pdf:0:0', 'data/CV Serge Keita.pdf:0:1', 'data/CV Serge Keita.pdf:0:2', 'data/CV Serge Keita.pdf:0:3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Oui, Serge a des comp√©tences en Python et AWS. Il est mentionn√© qu'il a travaill√© comme D√©veloppeur Python & Rust √† LEDR Technologies et est certifi√© AWS Solutions Architect Associate. Il poss√®de √©galement des exp√©riences professionnelles impliquant le d√©ploiement de solutions informatiques sur AWS et l'utilisation d'outils tels que Terraform pour la conception d'architectures AWS.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag2(query_text: str):\n",
    "    # Prepare the DB.\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    # print(prompt)\n",
    "\n",
    "    #pipeline(\"text-generation\", model=\"tiiuae/falcon-7b-instruct\", trust_remote_code=True)\n",
    "    llm = pipeline(\"text-generation\", model=\"gpt2\", framework=\"pt\")  # force PyTorch framework\n",
    "\n",
    "    #model = Ollama(model=\"openhermes\")\n",
    "    #response_text = model.invoke(prompt)\n",
    "    response = llm(prompt, max_length=1000)\n",
    "    response_text = response[0][\"generated_text\"]\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 4, updating n_results = 4\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "COMP√âTENCESS E R G E\n",
      "KEITAData Enthousiaste | Machine Learning | Devops\n",
      "Cloud | Python D√©veloppeur\n",
      "POINTS FORTS+33698480507 @  sergekeita01@gmail.com \n",
      "CERTIFICATIONS\n",
      "AWS Cloud Quest : Machine\n",
      "Learning\n",
      "AWS Solutions Architect Associate \n",
      "HarshiCorp Terraform Associate 003\n",
      "Certification CISCO CCNA1 \n",
      "Certification AZ 900 (2022)\n",
      "TOEICData Scientest\n",
      "English (Fluent)\n",
      "French (Fluent)LANGUAGESEXPERIENCES\n",
      "FORMATIONSPROFILE\n",
      "Fort d'une exp√©rience internationale et d‚Äôun master en Big Data & Artificial\n",
      "Intelligence, j'ai acquis de solides connaissances en programmation, machine\n",
      "learning et le d√©ploiement de solutions informatiques sur AWS. Passionn√© par\n",
      "la data, j'aime explorer de nouvelles id√©es et proposer des solutions cr√©atives\n",
      "pour relever les d√©fis du domaine.\n",
      "\n",
      "---\n",
      "\n",
      "pour relever les d√©fis du domaine.\n",
      "12/2023 - 04/2024 Consultant Devops Cloud AWS\n",
      "Data Scientest X Storm Reply - POEI\n",
      "02/2023 - 08/2023 Consultant Data & Cyber\n",
      "HEADMIND PARTNERS\n",
      "08/2022 - 01/2023 Data Scientist\n",
      "DATAXLABConception d'architectures AWS pour le d√©ploiement de services web\n",
      "Terraform & services AWS : ECS, RDS, CLOUDFRONT, ALB, EFS, S3 ...\n",
      "GitHub Action & Pytest\n",
      "Kubernetes / Docker / FastAPI & Linux\n",
      "Programmation d‚Äôun outil d‚Äôanalyse de risque s√©curit√©/d‚Äôaudit\n",
      "Impl√©mentation et entrainement d'un model de classification de\n",
      "fichier confidentiel DLP \n",
      "Cr√©ation d‚Äôun model d'analyse de risque s√©curit√© bas√© sur du Natural\n",
      "Language Processing (NLP) \n",
      "D√©veloppement d‚Äôun model pour de la d√©tection de bact√©ries.\n",
      "Collecte, nettoyage et pr√©paration des donn√©es\n",
      "\n",
      "---\n",
      "\n",
      "Collecte, nettoyage et pr√©paration des donn√©es\n",
      "Cr√©ation du model de segmentation d'image non supervis√© (WNET)\n",
      "Analyse et pr√©sentation des r√©sultats du model - Anglais\n",
      "06/2022  - 08/2022 D√©veloppeur Python & Rust\n",
      "LEDR TECHNOLOGIES\n",
      "Programmation de fonctions en Python\n",
      "Migration d‚Äôune grande partie des fonctions en RustPARIS\n",
      "PARIS\n",
      "LAS VEGAS\n",
      "LOS ANGELES\n",
      "EFREI Paris\n",
      "Universit√© Claude Bernard Lyon 1Devops Cloud Engineer\n",
      "Big Data & AI Engineer\n",
      "Lincence Informatique2023 - 2024\n",
      "PARIS\n",
      "2020 - 2023\n",
      "PARIS\n",
      "2017 - 2020\n",
      "LYONProgrammations : Python, Scala,\n",
      "Java, Rust\n",
      "Machine/Deep learning  : NLP,\n",
      "KNN, K-means, SVM, Linear &\n",
      "logistic regression ...\n",
      "SQL & NoSQL : MySQL,\n",
      "MariaDB, MongoDB \n",
      "FastAPI, Administration linux\n",
      "GitHub & Github Action / Gitlab\n",
      "Docker, Kubernetes, Ansible,\n",
      "Terraform, AWS\n",
      "\n",
      "---\n",
      "\n",
      "GitHub & Github Action / Gitlab\n",
      "Docker, Kubernetes, Ansible,\n",
      "Terraform, AWS\n",
      "MLflow, Streamlit, Gen AITravail d‚Äô√©quipeR√©solution de probl√®me\n",
      "Collaborer en √©quipe pour\n",
      "mettre en ≈ìuvre des solutions\n",
      "datacomprendre les besoins m√©tier\n",
      "et proposer des solutions\n",
      "techniques appropri√©esCapacit√© d‚Äôanalyse\n",
      "capacit√© √† analyser des donn√©es\n",
      "complexes, √† identifier des\n",
      "tendances significativesp o r t f o l i o\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: Serge √† t il des comp√©tence en python et AWS ?\n",
      "\n",
      "FORTRANO NATIONAL ASSOCIATION\n",
      "\n",
      "AWS, AWS - Automata, Big Data Dev\n",
      "Sources: ['data/CV Serge Keita.pdf:0:0', 'data/CV Serge Keita.pdf:0:1', 'data/CV Serge Keita.pdf:0:2', 'data/CV Serge Keita.pdf:0:3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Human: \\nAnswer the question based only on the following context:\\n\\nCOMP√âTENCESS E R G E\\nKEITAData Enthousiaste | Machine Learning | Devops\\nCloud | Python D√©veloppeur\\nPOINTS FORTS+33698480507 @  sergekeita01@gmail.com \\nCERTIFICATIONS\\nAWS Cloud Quest : Machine\\nLearning\\nAWS Solutions Architect Associate \\nHarshiCorp Terraform Associate 003\\nCertification CISCO CCNA1 \\nCertification AZ 900 (2022)\\nTOEICData Scientest\\nEnglish (Fluent)\\nFrench (Fluent)LANGUAGESEXPERIENCES\\nFORMATIONSPROFILE\\nFort d'une exp√©rience internationale et d‚Äôun master en Big Data & Artificial\\nIntelligence, j'ai acquis de solides connaissances en programmation, machine\\nlearning et le d√©ploiement de solutions informatiques sur AWS. Passionn√© par\\nla data, j'aime explorer de nouvelles id√©es et proposer des solutions cr√©atives\\npour relever les d√©fis du domaine.\\n\\n---\\n\\npour relever les d√©fis du domaine.\\n12/2023 - 04/2024 Consultant Devops Cloud AWS\\nData Scientest X Storm Reply - POEI\\n02/2023 - 08/2023 Consultant Data & Cyber\\nHEADMIND PARTNERS\\n08/2022 - 01/2023 Data Scientist\\nDATAXLABConception d'architectures AWS pour le d√©ploiement de services web\\nTerraform & services AWS : ECS, RDS, CLOUDFRONT, ALB, EFS, S3 ...\\nGitHub Action & Pytest\\nKubernetes / Docker / FastAPI & Linux\\nProgrammation d‚Äôun outil d‚Äôanalyse de risque s√©curit√©/d‚Äôaudit\\nImpl√©mentation et entrainement d'un model de classification de\\nfichier confidentiel DLP \\nCr√©ation d‚Äôun model d'analyse de risque s√©curit√© bas√© sur du Natural\\nLanguage Processing (NLP) \\nD√©veloppement d‚Äôun model pour de la d√©tection de bact√©ries.\\nCollecte, nettoyage et pr√©paration des donn√©es\\n\\n---\\n\\nCollecte, nettoyage et pr√©paration des donn√©es\\nCr√©ation du model de segmentation d'image non supervis√© (WNET)\\nAnalyse et pr√©sentation des r√©sultats du model - Anglais\\n06/2022  - 08/2022 D√©veloppeur Python & Rust\\nLEDR TECHNOLOGIES\\nProgrammation de fonctions en Python\\nMigration d‚Äôune grande partie des fonctions en RustPARIS\\nPARIS\\nLAS VEGAS\\nLOS ANGELES\\nEFREI Paris\\nUniversit√© Claude Bernard Lyon 1Devops Cloud Engineer\\nBig Data & AI Engineer\\nLincence Informatique2023 - 2024\\nPARIS\\n2020 - 2023\\nPARIS\\n2017 - 2020\\nLYONProgrammations : Python, Scala,\\nJava, Rust\\nMachine/Deep learning  : NLP,\\nKNN, K-means, SVM, Linear &\\nlogistic regression ...\\nSQL & NoSQL : MySQL,\\nMariaDB, MongoDB \\nFastAPI, Administration linux\\nGitHub & Github Action / Gitlab\\nDocker, Kubernetes, Ansible,\\nTerraform, AWS\\n\\n---\\n\\nGitHub & Github Action / Gitlab\\nDocker, Kubernetes, Ansible,\\nTerraform, AWS\\nMLflow, Streamlit, Gen AITravail d‚Äô√©quipeR√©solution de probl√®me\\nCollaborer en √©quipe pour\\nmettre en ≈ìuvre des solutions\\ndatacomprendre les besoins m√©tier\\net proposer des solutions\\ntechniques appropri√©esCapacit√© d‚Äôanalyse\\ncapacit√© √† analyser des donn√©es\\ncomplexes, √† identifier des\\ntendances significativesp o r t f o l i o\\n\\n---\\n\\nAnswer the question based on the above context: Serge √† t il des comp√©tence en python et AWS ?\\n\\nFORTRANO NATIONAL ASSOCIATION\\n\\nAWS, AWS - Automata, Big Data Dev\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag2(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
